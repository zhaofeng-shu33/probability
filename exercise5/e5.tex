\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[thehwcnt=5]{iidef}
\DeclareMathOperator{\bP}{\mathbb{P}}
\DeclareMathOperator{\Bern}{Bern}
%\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\mF}{\mathcal{F}}
\usepackage[utf8]{inputenc}
\thecourseinstitute{Tsinghua-Berkeley Shenzhen Institute}
%\title{ldp-1}
%\author{zhaof17 }
%\date{March 2021}
\thecoursename{Probability}
\theterm{Spring 2021}
%\setlist[enumerate,2]{label=(\roman*)}
\begin{document}

\courseheader
\name{Feng Zhao}

\begin{enumerate}
\item The answer is yes. $X+Y$ may not be Gaussian
distribution. 
Let $X\sim \mathcal{N}(0,1)$ and $Y=(-1)^Z X$ where $Z \sim \Bern(\frac{1}{2})$ and $Z$ is independent with $X$. We can verify $Y$ follows standard normal
distribution by the symmetric property of
normal distribution. However, $P(X+Y=0)=\frac{1}{2}$, which implies that
$X+Y$ can not be Gaussian distribution.

\item $X \sim \mathcal{N}(0,1)$, then $Z=(X+Y)/\sqrt{2}$ has the same distribution as
$X$. Let $\varphi(t)$ be the common characteristic
function of $X$ and $Z$. Since $\varphi(t)=
\E[\exp(it\frac{X+Y}{\sqrt{2}})]
=\E[\exp(it\frac{X}{\sqrt{2}})]
\E[\exp(it\frac{Y}{\sqrt{2}})]=
\varphi^2(\frac{t}{\sqrt{2}})$.
Then we have the identity $\varphi(t)=\varphi^2(\frac{t}{\sqrt{2}})$.
Let $f(t) = \ln \varphi(t)$. Then $f(t)$
satisfies $f(t)=2f(\frac{t}{\sqrt{2}})$.
$f(0)=\ln \varphi(0)=0$.
Taking the derivative on both sides and let
$t=0$, we get $f'(0)=0$. Recursively applying
the formula, we get $f(t) = 2^{2n} f(\frac{t}{2^{n}})$. For fixed $t$,
we expand $f(\frac{t}{2^{n}})$ at zero:
$f(t) = \frac{1}{2}f''(0)t^2 + \frac{1}{6}f^{(3)}(\xi)\frac{t^3}{2^n}$
where $\xi \in (0, \frac{t}{2^{n}})$.
Let $n\to \infty$, we have $f(t)=\frac{1}{2}f''(0)t^2$.
Therefore, $\varphi(t)=\exp(Ct^2)$ for some constant $C$.
Since $\E[X^2]=-\frac{1}{2}\varphi''(t)|_{t=0}=1 \implies C=-\frac{1}{2}$. The characteristic function is indeed
that of the standard normal distribution. By the uniqueness
of the characteristic function, we conclude that $X$
follows standard normal distribution.

\item we can write the pdf of $(X_1, \dots, X_{2n-1})$
as 
$$
f_{X_1, \dots, X_{2n-1}}(x_1, \dots, x_{2n-1})
= c_n \exp(-\frac{1}{2} x^T A x)
$$
By the definition of $f$, $A$ is a positive 
definite matrix. Therefore, $(X_1, \dots, X_{2n-1})$
follow jointly Gaussian distributions. The determinant of $A$ is:
\begin{align*}
    |A|=|\begin{pmatrix}
    1 & -1 & 0 & \dots \\
    -1 & 2 & 1 & \dots \\
    \vdots & \vdots & \vdots & \vdots \\
    \dots & -1 & 2 & -1 \\
    \dots & 0 & -1 & 2
    \end{pmatrix}|
    =|\begin{pmatrix}
    1 & -1 & 0 & \dots \\
    0 & 1 & 1 & \dots \\
    \vdots & \vdots & \vdots & \vdots \\
    \dots & 0 & 1 & -1 \\
    \dots & 0 & 0 & 1
    \end{pmatrix}| = 1
\end{align*}
Therefore $c_n = (2\pi)^{1/2 - n}$. Besides,
$\Var[X_n] = 2$.
\item
\begin{enumerate}[label=(\roman*)]
    \item The answer is yes. $\{S_n / n \}_{n=1}^{\infty}$
    converge in distribution to Cauchy distribution
    with PDF $f(x)=\frac{1}{\pi (1+x^2)}$.
    The characteristic function of $S_n$
    is $\varphi(t)=\exp(-n|t|)$. Then the characteristic
    function of $S_n/n$ is $\varphi(t)=\exp(-|t|)$.
    Therefore, $S_n/n$ follows Cauchy distribution with
    parameter $1$.
    \item The answer is yes. $\{S_n / n^2 \}_{n=1}^{\infty}$
    converge in distribution to $0$. The characteristic
    function of $S_n/n$ is $\varphi(t)=\exp(-|t|/n)$, which
    converges to 1 for all given $t$. The constant $1$
    is the characteristic function of random variable 0.
    By LÃ©vy's continuity theorem, we have the aforementioned claim.
    \item The answer is no. Since Cauchy distribution
    does not have finite mean, Central Limit Theorem
    cannot be applied. $\{S_n / \sqrt{n} \}_{n=1}^{\infty}$
    does not converge in distribution.
\end{enumerate}
\item By Taylor
expansion, we have $-\frac{1}{2} x^2-\frac{1}{12}  x^4 \leq \log \cos x \leq -\frac{1}{2} x^2$ for $|x|<0.5$.
We claim that $\beta_0 = \sqrt{2}$. The
characteristic function $X_j$ is $\varphi_j(t)=\E[\exp(iX_j t)]
=\frac{e^{itj}+e^{-itj}}{2}=\cos (j t)$. The the characteristic
function for $S_n$ is $\phi_n(t)=\prod_{j=1}^{n} \cos(jt)$.
\begin{enumerate}
    \item $\{S_n / n^2 \}_{n=1}^{\infty}$
    converge to $0$ in distribution. To prove that
    we need only to show that
    $\lim_{n\to \infty}\phi_n(\frac{t}{n^2}) \to 1$ for any given $t$, which is equivalent to show
    that
    $\lim_{n\to \infty} \sum_{j=1}^{n} \log \cos(\frac{jt}{n^2}) = 0$. It is obvious that
    $\sum_{j=1}^{n} \log \cos(\frac{jt}{n^2}) \leq 0$.
    On the other hand, we use Taylor expansion:
    $\sum_{j=1}^{n} \log \cos(\frac{jt}{n^2})
    \geq -\frac{1}{2}\sum_{j=1}^{n} \frac{j^2t^2}{n^4}
    -\frac{1}{12}\sum_{j=1}^{n} \frac{j^4t^4}{n^8}=O(\frac{1}{n}) \to 0$.
    \item $\{S_n / n^{3/2} \}_{n=1}^{\infty}$
    converge to $\mathcal{N}(0, \frac{1}{3})$ in distribution.
    We use similar method with (a) to show this fact.
    We only need to show that $\lim_{n\to \infty}\phi_n(\frac{t}{n^{3/2}}) \to \exp(-\frac{1}{6}t^2)$,
    which is further equivalent with
    $\lim_{n\to \infty} \sum_{j=1}^{n} \log \cos(\frac{jt}{n^{3/2}}) = -\frac{1}{6}t^2$.
    Notice that
    \begin{align*}
     -\frac{t^2}{2n^3}\sum_{j=1}^n j^2 -\frac{t^4}{12n^6}\sum_{j=1}^n j^4   \leq \sum_{j=1}^{n} \log \cos(\frac{jt}{n^{3/2}})
        \leq -\frac{t^2}{2n^3}\sum_{j=1}^n j^2
    \end{align*}
    Since $\frac{1}{n^6}\sum_{j=1}^n j^4 = O(\frac{1}{n})$
    while $\frac{1}{n^3}\sum_{j=1}^n \sim \frac{1}{3}$,
    we reach
    $\lim_{n\to \infty} \sum_{j=1}^{n} \log \cos(\frac{jt}{n^{3/2}}) = -\frac{1}{6}t^2$.
    \item Let $\Phi$ be the CDF of Gaussian random variable
    with variance $\frac{1}{3}$.
    $\lim_{n\to \infty} P(\frac{S_n}{n} \leq x)
    =\lim_{n\to \infty} P(\frac{S_n}{n^{3/2}} \leq \frac{x}{\sqrt{n}}) = \Phi(0) = \frac{1}{2}$.
\end{enumerate}

\item 
\begin{enumerate}
    \item Yes, $\{S_n\}_{n=1}^{\infty}$ converges almost
    surely to some random variable. For any $w\in \Omega$,
    $S_n(w)$ is an increasing sequence. Therefore,
    the event that $\{S_n\}_{n=1}^{\infty}$ does not converge
    is equivalent to $S_n = \infty$.
    $P(S_n = \infty) \leq P(X_i=1 \textrm{ for infinite } i) = 0$. Since $S_n$ are random variables, $\lim_{n} S_n$ is also a random variable.
    $\E[Y_n]=\frac{1}{2^n}\sum_{i=1}^n \frac{1}{2^i}\binom{n}{i}=(\frac{3}{4})^n$. Then
    $\E[S_n]=\sum_{i=1}^n \E[Y_i] =\sum_{i=1}^n (\frac{3}{4})^i \implies \E[S]=\sum_{i=1}^{\infty} (\frac{3}{4})^i=3$ where $S=\lim_{n\to\infty} S_n$.
    To calculate $\Var[S]$, first we have
    $\E[Y_nY_{n+m}]=\E[X^2_1]^n[X_1]^m=(\frac{5}{8})^n(\frac{3}{4})^m$. Then $\E[S_n^2] = \sum_{i=1}^n (\frac{5}{8})^i
    + 2 \sum_{i=1}^n (\frac{5}{8})^i \sum_{j=i+1}^n
    (\frac{3}{4})^{j-i}=7\sum_{i=1}^n (\frac{5}{8})^i-6(\frac{3}{4})^{n}\sum_{i=1}^n (\frac{5}{6})^i$. Finally $\Var[S]=\E[S^2]-\E[S]^2=
    \lim_{n\to \infty} \E[S_n^2] - 9 = 
   7\sum_{i=1}^{\infty} (\frac{5}{8})^i-9=\frac{8}{3} $
   \item By the same argument as above, in such case
   $\{S_n\}_{n=1}^{\infty}$ also converge almost surely to some limit random variable.
\end{enumerate}
\end{enumerate}

\end{document}

